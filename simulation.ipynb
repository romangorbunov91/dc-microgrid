{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a7604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ===========================\n",
    "# 2. Safe Action Mechanism (DEAA)\n",
    "# ===========================\n",
    "class SafeActionMechanism:\n",
    "    def __init__(self, eta=0.5, phi=0.2, U_ref=50.0):\n",
    "        self.eta = eta\n",
    "        self.phi = phi\n",
    "        self.L = U_ref\n",
    "        self.T = 0.0\n",
    "\n",
    "    def smooth(self, raw_action):\n",
    "        L_new = self.eta * raw_action + (1 - self.eta) * (self.L + self.T)\n",
    "        T_new = self.phi * (L_new - self.L) + (1 - self.phi) * self.T\n",
    "        safe = L_new + T_new\n",
    "        self.L, self.T = L_new, T_new\n",
    "        return safe\n",
    "\n",
    "# ===========================\n",
    "# 3. Reward Function (Algorithm 1)\n",
    "# ===========================\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 4. State Normalizer\n",
    "# ===========================\n",
    "class RunningNormalizer:\n",
    "    def __init__(self, shape):\n",
    "        self.mean = np.zeros(shape, dtype=np.float32)\n",
    "        self.var = np.ones(shape, dtype=np.float32)\n",
    "        self.count = 1e-4\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "\n",
    "        delta = batch_mean - self.mean\n",
    "        tot_count = self.count + batch_count\n",
    "        self.mean += delta * batch_count / tot_count\n",
    "        m_a = self.var * self.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / tot_count\n",
    "        self.var = M2 / tot_count\n",
    "        self.count = tot_count\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x - self.mean) / (np.sqrt(self.var) + 1e-8)\n",
    "\n",
    "# ===========================\n",
    "# 5. SMAPPO Agent\n",
    "# ===========================\n",
    "class SMAPPO:\n",
    "    def __init__(self, obs_dim, action_dim, global_dim, lr_actor=1.5e-4, lr_critic=1e-4,\n",
    "                 clip_eps=0.2, ent_coef=0.01, gamma=0.8, gae_lambda=0.95):\n",
    "        self.actor = Actor(obs_dim, action_dim).to(device)\n",
    "        self.critic = Critic(global_dim).to(device)\n",
    "        self.actor_opt = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_opt = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        self.clip_eps = clip_eps\n",
    "        self.ent_coef = ent_coef\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones, next_value):\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            if i == len(rewards) - 1:\n",
    "                next_val = next_value\n",
    "            else:\n",
    "                next_val = values[i + 1]\n",
    "            delta = rewards[i] + self.gamma * next_val * (1 - dones[i]) - values[i]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[i]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32, device=device)\n",
    "        returns = advantages + torch.tensor(values, dtype=torch.float32, device=device)\n",
    "        return advantages, returns\n",
    "\n",
    "    def update(self, data, epochs=10, batch_size=64):\n",
    "        obs = torch.tensor(data['obs'], dtype=torch.float32, device=device)\n",
    "        global_obs = torch.tensor(data['global_obs'], dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(data['actions'], dtype=torch.float32, device=device)\n",
    "        logprobs = torch.tensor(data['logprobs'], dtype=torch.float32, device=device)\n",
    "        returns = data['returns']\n",
    "        advantages = data['advantages']\n",
    "\n",
    "        n = len(obs)\n",
    "        indices = np.arange(n)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, n, batch_size):\n",
    "                idx = indices[start:start+batch_size]\n",
    "                obs_b = obs[idx]\n",
    "                global_obs_b = global_obs[idx]\n",
    "                actions_b = actions[idx]\n",
    "                logprobs_b = logprobs[idx]\n",
    "                returns_b = returns[idx]\n",
    "                advantages_b = advantages[idx]\n",
    "\n",
    "                # Normalize advantages\n",
    "                advantages_b = (advantages_b - advantages_b.mean()) / (advantages_b.std() + 1e-8)\n",
    "\n",
    "                # Critic loss\n",
    "                values_pred = self.critic(global_obs_b)\n",
    "                critic_loss = F.mse_loss(values_pred, returns_b)\n",
    "\n",
    "                self.critic_opt.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_opt.step()\n",
    "\n",
    "                # Actor loss\n",
    "                mean_new, log_std_new = self.actor(obs_b)\n",
    "                std_new = log_std_new.exp()\n",
    "                dist_new = Normal(mean_new, std_new)\n",
    "                logp_new = dist_new.log_prob(actions_b).sum(-1, keepdim=True)\n",
    "                logp_new -= (2 * (np.log(2) - torch.tanh(actions_b) - F.softplus(-2 * torch.tanh(actions_b)))).sum(-1, keepdim=True)\n",
    "\n",
    "                ratio = torch.exp(logp_new - logprobs_b)\n",
    "                surr1 = ratio * advantages_b\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advantages_b\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                entropy = dist_new.entropy().sum(-1).mean()\n",
    "                actor_loss -= self.ent_coef * entropy\n",
    "\n",
    "                self.actor_opt.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_opt.step()\n",
    "\n",
    "# ===========================\n",
    "# 6. Имитация среды (замените на реальную)\n",
    "# ===========================\n",
    "def mock_env_step(safe_actions, n_agents=4):\n",
    "    # Моделируем изменения напряжения и тока\n",
    "    base_U = 50.0\n",
    "    base_I = np.random.uniform(-10, 10, n_agents)\n",
    "    U = base_U + np.random.normal(0, 0.5, n_agents) + 0.01 * np.sum(safe_actions - base_U)\n",
    "    I = base_I + np.random.normal(0, 0.2, n_agents)\n",
    "\n",
    "    # Наблюдения: [I_self, I_neigh1, I_neigh2, U_self, U_neigh1, U_neigh2]\n",
    "    obs_list = []\n",
    "    for i in range(n_agents):\n",
    "        I_self = I[i]\n",
    "        U_self = U[i]\n",
    "        I_n = [I[(i-1)%n_agents], I[(i+1)%n_agents]]\n",
    "        U_n = [U[(i-1)%n_agents], U[(i+1)%n_agents]]\n",
    "        obs = np.array([I_self] + I_n + [U_self] + U_n, dtype=np.float32)\n",
    "        obs_list.append(obs)\n",
    "\n",
    "    global_obs = np.concatenate(obs_list, dtype=np.float32)\n",
    "    local_obs = np.stack(obs_list)\n",
    "    done = False\n",
    "    return local_obs, global_obs, U, I, done\n",
    "\n",
    "# ===========================\n",
    "# 7. Основной цикл обучения\n",
    "# ===========================\n",
    "if __name__ == \"__main__\":\n",
    "    n_agents = 4\n",
    "    obs_dim = 6\n",
    "    action_dim = 1\n",
    "    global_dim = n_agents * obs_dim\n",
    "    max_steps = 400\n",
    "    tau = 20\n",
    "    batch_size = 168\n",
    "\n",
    "    agent = SMAPPO(obs_dim, action_dim, global_dim, batch_size=batch_size)\n",
    "    safe_mechs = [SafeActionMechanism() for _ in range(n_agents)]\n",
    "    state_norm = RunningNormalizer(obs_dim)\n",
    "\n",
    "    num_episodes = 1000\n",
    "    for ep in range(num_episodes):\n",
    "        # Reset\n",
    "        local_obs, global_obs, _, _, _ = mock_env_step(np.full(n_agents, 50.0))\n",
    "        count = 0\n",
    "        buffer = {k: [] for k in ['obs', 'global_obs', 'actions', 'logprobs', 'rewards', 'dones', 'values']}\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            # Нормализуем входы\n",
    "            local_obs_norm = np.array([state_norm.normalize(o) for o in local_obs])\n",
    "            actions_raw = []\n",
    "            logprobs_raw = []\n",
    "            safe_actions = []\n",
    "\n",
    "            for i in range(n_agents):\n",
    "                o = torch.tensor(local_obs_norm[i:i+1], device=device)\n",
    "                a, logp = agent.actor.get_action(o)\n",
    "                a_cpu = a.cpu().item()\n",
    "                a_v = a_cpu * 10 + 50  # [-1,1] → [40,60]\n",
    "                a_safe = safe_mechs[i].smooth(a_v)\n",
    "                actions_raw.append(a_cpu)\n",
    "                logprobs_raw.append(logp.item())\n",
    "                safe_actions.append(a_safe)\n",
    "\n",
    "            # Шаг среды\n",
    "            next_local, next_global, voltages, currents, done = mock_env_step(safe_actions)\n",
    "            reward, count = compute_reward(voltages, currents, count, tau)\n",
    "\n",
    "            # Сохраняем значения value для GAE\n",
    "            with torch.no_grad():\n",
    "                val = agent.critic(torch.tensor(global_obs, dtype=torch.float32, device=device)).item()\n",
    "\n",
    "            buffer['obs'].append(local_obs.copy())\n",
    "            buffer['global_obs'].append(global_obs.copy())\n",
    "            buffer['actions'].append(actions_raw)\n",
    "            buffer['logprobs'].append(logprobs_raw)\n",
    "            buffer['rewards'].append(reward)\n",
    "            buffer['dones'].append(done)\n",
    "            buffer['values'].append(val)\n",
    "\n",
    "            local_obs, global_obs = next_local, next_global\n",
    "\n",
    "            # Обновляем нормализатор\n",
    "            state_norm.update(local_obs)\n",
    "\n",
    "        # Последнее значение для GAE\n",
    "        with torch.no_grad():\n",
    "            last_val = agent.critic(torch.tensor(global_obs, dtype=torch.float32, device=device)).item()\n",
    "\n",
    "        # Вычисляем GAE и returns\n",
    "        advantages, returns = agent.compute_gae(\n",
    "            buffer['rewards'], buffer['values'], buffer['dones'], last_val\n",
    "        )\n",
    "\n",
    "        # Подготавливаем данные\n",
    "        train_data = {\n",
    "            'obs': np.array(buffer['obs']),\n",
    "            'global_obs': np.array(buffer['global_obs']),\n",
    "            'actions': np.array(buffer['actions']),\n",
    "            'logprobs': np.array(buffer['logprobs']),\n",
    "            'returns': returns,\n",
    "            'advantages': advantages\n",
    "        }\n",
    "\n",
    "        # Обновляем агента\n",
    "        agent.update(train_data, batch_size=batch_size)\n",
    "\n",
    "        if ep % 20 == 0:\n",
    "            avg_r = np.mean(buffer['rewards'])\n",
    "            print(f\"Episode {ep}, Avg Reward: {avg_r:.3f}\")\n",
    "\n",
    "    print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
